{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Ideas for building custom Datasets for Text \n",
    "\n",
    "The idea is to convert the text we have into numerical value.\n",
    "1. We have an index and we need a vocabulory mapping of each word to index.\n",
    "2. We setup a pytorch dataset to load the data.\n",
    "3. The sequence length must be same for all the batches, so we make padding of every batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import pandas as pd \n",
    "import spacy  # we use spacy for implementation of tokenizer.\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence  # padding of batch.\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image  # Load imgage\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulory class is created for mapping each word to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load('en_core_web_sm') # to know tokenizer it is working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_threshold tells, if the word isn't repeating those frequent amount of time, we can ignore it.\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} # Padd Token , Start of sentence, End of sentence, Unknown.\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self): # getting length of our vocabulory.\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)] # we get lower case of the tokenizer of text we send.\n",
    "        # example:>  \"Get along soon\" -> [\"get\",\"along\",\"soon\"]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list): # used to go through each of text and see if its over the threshold and if so we ignore it.\n",
    "        frequencies = {}\n",
    "        idx = 4  # we are starting with an index of 4 because we already included first three.\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies: \n",
    "                    frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    frequencies[word] += 1 \n",
    "\n",
    "                if frequencies[word] == self.freq_threshold: # here we see if frequency of word is equad to the threshold frequency.\n",
    "                    self.stoi[word] = idx # So we set the index starting at 4.\n",
    "                    self.itos[idx] = word # and we set word into that index.\n",
    "                    idx += 1 # we increment the index.\n",
    "\n",
    "    def numericalize(self, text): # we take the sentence and convert them to numerical values.\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] # if token are in stoi, then it surparses the frequency threshold.\n",
    "                                             #Else it wouldnt be in self.toi and we just return the index of unkown token.\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset): # Talking the class dataset.\n",
    "    \n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5): # root directory of image is passed with caption file and a fequency threshold.\n",
    "        self.root_dir = root_dir # getting the root directory, in our case, we have flickr8k folder.\n",
    "        self.df = pd.read_csv(captions_file) # reading the captions from caption file.\n",
    "        self.transform = transform # \n",
    "\n",
    "        \n",
    "        self.imgs = self.df[\"image\"]  # we get the image from image column.\n",
    "        self.captions = self.df[\"caption\"] # we get the caption assosiated with image from image column.\n",
    "\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold) # Initialize vocabulary with respect to threshold we specified.\n",
    "        self.vocab.build_vocabulary(self.captions.tolist()) # We build the vocabulory here and the captions is passed as a list into the function's parameters.\n",
    "\n",
    "    def __len__(self): # we get length of dataframe here.\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index): # we use to get a single example with corresponding caption.\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\") # loading of image.\n",
    "\n",
    "        if self.transform is not None: # in case we have a stransform we can use it.\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]] # string to index -> index of start token.\n",
    "        numericalized_caption += self.vocab.numericalize(caption) # we numericalize the caption.\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"]) # append end of sentence.\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption) #return image by converting numnericalized caption to tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding of data to make sure that batches are of equal dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sequence mode, the sequence length must all be same. But caption length may be different for different examples.\n",
    "\n",
    "Check the maximum length of longest sentence and padd to that length , but could be unnecessary computation.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx # getting the padd index.\n",
    "\n",
    "    def __call__(self, batch): # we have batch, which is list of all examples we have.\n",
    "        # unsqueze -> Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch] # 1st item returned for each item in batch.\n",
    "        # torch.cat -> Concatenates the given sequence of seq tensors in the given dimension. \n",
    "        imgs = torch.cat(imgs, dim=0) #Concates the images we unsquezed to given dimension. All images must be of same size.\n",
    "        targets = [item[1] for item in batch] # targets are the captions.\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx) # targets are papdded with pad_sequence function.\n",
    "        # if batch_first = True,  If True, then the input and output tensors are provided as (batch, seq, feature).\n",
    "\n",
    "        return imgs, targets # images and targets are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loader class to load the file and process everything with classes defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(  # loads everything for us.\n",
    "    root_folder,\n",
    "    annotation_file,\n",
    "    transform,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming and defining the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)), transforms.ToTensor(),]\n",
    "    )\n",
    "\n",
    "    loader, dataset = get_loader(\n",
    "        \"flickr8k/images/\", \"flickr8k/captions.txt\", transform=transform\n",
    "    )\n",
    "\n",
    "    for idx, (imgs, captions) in enumerate(loader):\n",
    "        print(imgs.shape)\n",
    "        print(captions.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
