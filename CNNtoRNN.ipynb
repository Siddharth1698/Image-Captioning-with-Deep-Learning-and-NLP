{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import statistics\n",
    "import torchvision.models as models ## used to load the pytorch models for vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We define a module that link our CNN to RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module): # cnn to rnn is hooked here.\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    # In decoder rnn we are making some target caption for every timestep.The predictions are used in our loss function. \n",
    "    # Actual predictions are made from the dataset.\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50): # this is used so that we can use this to get predicted word.\n",
    "        result_caption = [] \n",
    "\n",
    "        with torch.no_grad(): \n",
    "            x = self.encoderCNN(image).unsqueeze(0) # we take imgae and send to encodder cnn a dimmension for batch.\n",
    "            states = None # hidden and cell states for lstm.\n",
    "\n",
    "            for _ in range(max_length): ## upto the max_length words prediction, 50 here and depends on dataset.\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)  # at beginning it will be initialized as 0.\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0)) # sending single image and unsquence for that batch dimension. \n",
    "                predicted = output.argmax(1) # so we taking word with higgest probablity.\n",
    "                result_caption.append(predicted.item()) \n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0) # taking the predicted word.\n",
    "                # now x is looped through and send as next word in RNN.\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\": # check if vocab is equal to end of sentence then break.\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_caption] # at end we are returning the string and not just the indices,\n",
    "                                                                #                           that correspond to the presictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
